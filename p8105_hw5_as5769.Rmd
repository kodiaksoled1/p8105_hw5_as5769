---
title: "Homework 5"
author: "Kodiak Soled"
date: "11/2/2019"
output: github_document
always_allow_html: yes
---

```{r setting up document, include = FALSE}
library(tidyverse)
library(patchwork)
library(viridis)
library(kableExtra)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d

scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

## Problem 1

#### Read in Data

First, I read in the missing data. 

```{r loading in missing dataset}
set.seed(10)

iris_with_missing = 
  iris %>% 
  map_df(~replace(.x, sample(1:150, 20), NA)) %>%
  mutate(Species = as.character(Species))
```

#### Create a Function to Address Missing Data

In order to fill in the missing data, I first needed to write a `function` that:

* Took a vector as an argument
* `Replace`d missing values:
    * of numeric variables with the mean of non-missing values
    * of character variables with `"virginica"`
* Returned the resulting vector

```{r write a function to replace missing values}
tidy_iris = function(x) {
  
  if (!is.numeric(x) & !is.character(x)) {
    stop("Argument x should be numeric or character")
  } else if (is.character(x)) {
    x = replace(x, is.na(x) == TRUE, "virginica")
  } else if (is.numeric(x)) {
    mean_x = mean(x, na.rm = TRUE)
    x = replace(x, is.na(x) == TRUE, mean_x)
  }
  
}
```

#### Create a Map Statement

Then, I applied this function to the `iris_with_missing` dataset using a `map` statement and `bind_rows` in order to combine the 5 datasets into a single one:

```{r write a map statement, warning = FALSE}
output = 
  map(iris_with_missing, tidy_iris) %>%
  bind_rows()
```

#### Table of New Iris Data

Here is a reader-friendly table the first 30 observations of the replaced iris dataset:

```{r resulting table}
head(output, n = 30) %>%
  janitor::clean_names() %>%
  knitr::kable(digits = 1, caption = "Iris Dataset (First 30 Observations)") %>%
  kable_styling(bootstrap_options = c("striped", "condensed", font_size = 12))
```

## Problem 2

#### Load in Data files

In order to create a tidy dataframe containing data from all participants, including the subject ID, arm, and observations over time, I first loaded in the 20 seperate datafiles using `list.files` (created a dataframe with the file names):

```{r}
study_data_2 = 
  list.files(path = "./data_hw5", full.names = FALSE) %>%
  tibble::enframe(name = NULL)
study_data_2
```

#### Create Tidy Dataframe

Then, I iterated over the file names and read in the data (`read_csv`) for each of the 20 subjects (from 20 strings to 1 string using `str_c`) using `purrr::map` (and saving the result as a new variable in the dataframe). I then `unnest`ed the variable "data" so I could see the measurements for each subject across the eight weeks. I then tidied the dataset so that there were `seperate` variables for study group (control vs. experimental) and subject id and removed the ".csv" in the name using `mutate` and `str_replace`. Finally, I put the dataframe into a reader-friendly table using `knitr::kable` and made sure all the measurements were rounded to 2 decimal places using `digits = 2`.

```{r write using map, message = FALSE}
df_study_data_2 = 
  study_data_2 %>%
  mutate(
    data = map(value, ~read_csv(str_c("./data_hw5/", .x)))
  ) %>%
  unnest(cols = data) %>%
  separate(value, into = c("arm", "subject_id"), sep = "_") %>%
  mutate(subject_id = str_replace(subject_id, ".csv", ""))
df_study_data_2 %>%
  knitr::kable(digits = 2, caption = "Tidy Study Dataframe") %>%
  kable_styling(bootstrap_options = c("striped", "condensed", font_size = 12))

```

#### Make Spaghetti Plots

Next, I used `pivot_longer` to tidy the data in a way that allowed me to make a spaghetti plot which showed observations on each subject over time:

```{r}
df = 
  df_study_data_2 %>%
  pivot_longer(
    cols = starts_with("week_"),
    names_to = "week",
    names_prefix = "week_",
    values_to = "observation"
    ) %>%
 mutate(
   arm = recode(arm,
                `con` = "Control",
                `exp` = "Experimental"
                )
   ) %>%
  rename(Group = arm)
```

My first graph I made allows you to see the two study groups on the same graph. We can see the differences between the two study arms by the two colors: 

```{r}
ggplot(df, aes(x = week, y = observation, group = subject_id, color = Group)) + 
  geom_path() +
  labs(
    title = "Observation of Each Study Subject over Eight Weeks By Study Group",
    x = "Week",
    y = "Observation",
    caption = "Data from HW 5 Problem 2 Zip File")
```

My second graph stratified the two study groups using `facet_grid`. In this graph we can see each of the 20 study participants as well as the differences between the two study groups:

```{r}
ggplot(df, aes(x = week, y = observation, group = subject_id, color = subject_id)) + 
  geom_line() + 
  facet_grid(~Group) +
  labs(
    title = "Observation of Each Study Subject over Eight Weeks By Study Group",
    x = "Week",
    y = "Observation",
    caption = "Data from HW 5 Problem 2 Zip File")
```

#### Description of Differences Between Groups

The control group seems to maintain the same measurements across the eight weeks whereas the experimental group seems to significantly increase their measurements across the eight weeks. It would appear that the independent variable in the study is making an positive impact on whatever dependent varaible it is trying to increase.

## Problem 3 

#### Make a Function

First, I made a function which fixed the following design elements:
  * n = 30
  * xi1 as draws from a standard Normal distribution
  * β0 = 2
  * σ2 = 50
and set β1 = 0. 

I then put the _lm_ for yi = β0 + β1xi1 + ϵi with ϵi∼N[0,σ2] inside the function and generated the estimate and p-value using the `broom::tidy` function
  
For each dataset, I saved (`select`ed) the estimate and p-value of β̂1 by `filter`ing for "x:

```{r}
set.seed(1)

sim_regression = function(beta1 = 0) {

  sim_data = tibble(
    x = rnorm(30, mean = 1, sd = 1),
    y = 2 + beta1 * x + rnorm(30, 0, sqrt(50))
  )

  ls_fit = lm(y ~ x, data = sim_data)
  
  broom::tidy(ls_fit) %>%
    filter(term == "x") %>%
    select(estimate, p.value)
  
}
```

#### Generate 10,0000 Datasets for β1 = 0

I then generated 10,000 datasets of β1 = 0 using `rerun` on my "sim_regression" function and used `bind_rows` to create one massive dataset from the list of 10,000 datasets:

```{r writing a map statement}
output = 
  rerun(10000, sim_regression(beta1 = 0)) %>%
  bind_rows()
```

The first 10 rows of this simulated dataset of β1 = 0 can be seen here:

```{r}
head(output, n = 10) %>%
  janitor::clean_names() %>%
  knitr::kable(digits = 3, caption = "First 10 Rows of Simulation Regression for β1 = 0") %>%
  kable_styling(bootstrap_options = c("striped", "condensed", font_size = 12))
```

#### Generate 10,0000 Datasets for β1 = {0, 1, 2, 3, 4, 5, 6}

I then repeated the above for β1 = {1, 2, 3, 4, 5, 6} by creating a `tibble` for each β1 value and used `map` to allow the function "sim_regression" to `rerun` 10,000 for each β1 value (.x = beta1). I again had to `bind_rows` to make the list into a massive dataset, and `unnest` the "output_df" variable to see all the values for each β1. Finally, I removed the "output_list" function to make the dataframe contain only the variables I needed:    

```{r}
sim_results = 
  tibble(
    beta1 = c(0, 1, 2, 3, 4, 5, 6)
    ) %>% 
  mutate(
    output_list = map(.x = beta1, ~ rerun(10000, sim_regression(beta1 = .x))),
    output_df = map(output_list, bind_rows)
    ) %>%
  unnest(output_df) %>%
  select(-output_list)
```

The first 10 rows of this simulated dataset of β1 = {0, 1, 2, 3, 4, 5, 6} can be seen here:

```{r}
head(sim_results, n = 10) %>%
  janitor::clean_names() %>%
  knitr::kable(digits = 3, caption = "First 10 Rows of Simulation Regression for β1 = {0, 1, 2, 3, 4, 5, 6}") %>%
  kable_styling(bootstrap_options = c("striped", "condensed", font_size = 12))
```

#### Plot #1

To show the proportion of times the null was rejected (the power of the test) on the y-axis and the true value of β1 on the x-axis I needed to `group_by` the β1's and `count` the number of times the p-value was < 0.05 (i.e., the null was rejected). I then created a "power" variable using `mutate` and dividing the number of times the null was rejected by the total sample of each β1 (sum(n)). I then cleaned up the variable names and used `filter` to only keep the proportions for the sample where the null was rejected: 

```{r null rejected}
null_sim_results =
  sim_results %>%
  group_by(beta1) %>% 
  count(p.value < 0.05) %>%
  mutate(power = n/sum(n)) %>%
  janitor::clean_names() %>%
  filter(p_value_0_05 == TRUE)
```

I then plotting this using `geom_point`: 

```{r message = FALSE}
ggplot(null_sim_results, aes(x = beta1, y = power)) +
  geom_point() +
  geom_smooth(se = FALSE) 
```

And made another plot using `geom_histogram` to better understand the association between the effect size and power:

```{r}
ggplot(null_sim_results, aes(x = beta1, y = power, fill = beta1)) +
  geom_histogram(stat = "identity")
```

#### Description of the Association Between Effect Size and Power

When a sample size is fixed (as in this example), as the effect size increases so does the power. This is visually displayed as a positive association between β1 and power. 

#### Plot #2

To show the average estimate of β̂1 on the y axis and the true value of β1 on the x axis I made a line plot. I did this by using `group_by` to organize the data by values of β1, taking the mean or average of each estimate using `summarize`, then using `geom_point` and `geom_line` within `ggplot` to create the graph:

```{r message = FALSE}
entire_sample = 
  sim_results %>%
  group_by(beta1) %>%
  summarize(mean_estimate = mean(estimate))

plot_entire = 
  ggplot(entire_sample, aes(x = beta1, y = mean_estimate)) + 
  geom_point(color = "blue") + 
  geom_line(color = "blue") +
  labs(
    title = "True Value of β1 vs. Average Est. \nof β̂1 for Entire Sample",
    x = "True Value of β1",
    y = "Average Est. of β̂1"
  )
```

#### Plot #3

To show the the average estimate of β̂1 *only in samples for which the null was rejected* on the y axis and the true value of β1 on the x axis I also used a line graph:

```{r message = FALSE}
null_rejected_sample = 
  sim_results %>%
  janitor::clean_names() %>%
  filter(p_value < 0.05) %>%
  group_by(beta1) %>%
  summarize(mean_estimate = mean(estimate))

plot_null = 
  ggplot(null_rejected_sample, aes(x = beta1, y = mean_estimate)) + 
  geom_point(color = "green") + 
  geom_line(color = "green") + 
  labs(
    title = "True Value of β1 vs. Average Est. \nof β̂1 for Rejected Nulls",
    x = "True Value of β1",
    y = "Average Est. of β̂1 among the Rejected Nulls"
  )
```

I then made a graph to show the plots of the two samples side by side. The `patchwork` library was loaded in at the beginning of this document so we can just combine (+) the two graph titles here to display the plots side by side:

```{r}
plot_entire + plot_null
```

My second graph shows the two samples overlayed ontop of one another. This one required more steps, but allows the viewer to better visualize the difference of the mean estimates between the two samples. I was able to do this by joining the two samples using `full_join`. Then I applied `geom_point` and `geom_line` to the mean estimate of each sample to make my combined `ggplot`: 

```{r overlaying graphs}
combined_data = 
  full_join(entire_sample, null_rejected_sample, by = "beta1") %>%
  janitor::clean_names()

ggplot(combined_data, aes(x = beta1)) + 
  geom_point(aes(y = mean_estimate_x), color = "blue") +
  geom_line(aes(y = mean_estimate_x), color = "blue") + 
  geom_point(aes(y = mean_estimate_y), color = "green") +
  geom_line(aes(y = mean_estimate_y), color = "green") +
  labs(
    title = "Sample average of β̂1 across tests for which the null is rejected \noverlayed with sample average of β̂1 across all tests",
    x = "True Value of β1",
    y = "Average Estimate"
  )
```

#### Answering if the Average of β̂1 across Tests is Approximately Equal Across the Two Samples

The sample average of β̂1 across tests for which the null is rejected (the green line) becomes approximately equal to the true value of β1 (the blue line) the closer it gets to β1 = 6 (from β1  = 1) because the effect size is largest when β1 = 6. The larger the effect size becomes, the more of the rejected nulls/p-values will be included in the sample and thus the mean estimates will become more similar to the true value of β1 (i.e., increased effect size = increased power).